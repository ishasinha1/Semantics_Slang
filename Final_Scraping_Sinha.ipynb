{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The files mentioned below are associated with a runtime and are deleted when one terminates. These files are attached on Canvas, for reference.\n",
        "\n",
        "This is the code I used to scrape the 'tags' on Tumblr for the data to be analysed. I make use of the `pytumblr` API, using the following information about its usage published by the official Tumblr account: [https://github.com/tumblr/pytumblr](https://). \n",
        "\n",
        "Further documentation is available at [https://pypi.org/project/PyTumblr2/](https://).\n",
        "\n",
        "As per the requirements of `pytumblr`, this project was registered as a Tumblr application to acquire oAuth consumer keys.\n",
        "\n",
        "Due to rate limitations of this API ([https://www.tumblr.com/docs/en/api/v2#authentication](https://)), it was run in sections, scraping several thousand posts for a given tag in every iteration. This code was run over the course of two days (4/27/23-4/29/23), on the first 100 words among the tags trending on Tumblr (found on Canvas under file name 'final.txt'). This list does not include proper nouns such as names of celebrities, TV shows or movies, and was manually compiled on 4/27/23, using Tumblr: [https://www.tumblr.com/explore/trending](https://).\n",
        "\n",
        "Upon completion, our data to be analysed contains over 70,000 instances of the usage of these words. It can be found on Canvas under 'tumblr.txt'."
      ],
      "metadata": {
        "id": "Pk07K-vrQPUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install pytumblr\n",
        "!pip install pytumblr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiZ6CH_MiQ24",
        "outputId": "2c200a45-94ad-4c75-e6f2-c0bd80b9cf39"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytumblr\n",
            "  Downloading PyTumblr-0.1.2-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pytumblr) (0.18.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from pytumblr) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->pytumblr) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->pytumblr) (2.27.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib->pytumblr) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib->pytumblr) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib->pytumblr) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib->pytumblr) (3.4)\n",
            "Installing collected packages: pytumblr\n",
            "Successfully installed pytumblr-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pytumblr, and other useful libraries\n",
        "import pytumblr\n",
        "import calendar\n",
        "import time\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "kzr44TBBigXD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33ff35bf-944e-4ae0-879d-3202109779b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate via OAuth\n",
        "client = pytumblr.TumblrRestClient(\n",
        "  'dq5cGucSj7Mxscb4KLfyeUwDEwTqIPvD8XY8NZJkXmHOmSoEKi',\n",
        "  'E0QO6nas5EckuTkxQGrvuvvWWpzHo5VvZ5kwYfDNI8OkuHH7Mc',\n",
        "  'jucbMZUhBq1qMMCD757SR3kgNYkfEYmcpYDbTNwg4f0sHmLhyp',\n",
        "  '1quwvGQmn21DFX6CkbF9xwCQ1ZTWfOnsnsLBDvFy3QCKHvCasR'\n",
        ")"
      ],
      "metadata": {
        "id": "Qk235SE0irXM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reads the contents of the file 'final.txt'\n",
        "# This file contains the 100 words under consideration.\n",
        "with open('final.txt') as f:\n",
        "  words = f.read()\n",
        "\n",
        "word_list = nltk.word_tokenize(words)"
      ],
      "metadata": {
        "id": "NlmIwzUdvZ7R"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrapes as many posts as possible for each tag and appends them to \n",
        "# 'tumblr.txt'\n",
        "def scrape(tag):\n",
        "\n",
        "  # Filter for the tags.\n",
        "  filter = 'raw' \n",
        "\n",
        "  # The posts from before the time at which the code was run are considered.\n",
        "  before = calendar.timegm(time.gmtime())\n",
        "  time_note = before\n",
        "\n",
        "  # Creates a reference to 'tumblr.txt', and appends the data to it.\n",
        "  f = open('tumblr.txt', 'a')\n",
        "\n",
        "  j = 0\n",
        "  # The following code is limited to being run 25 times due to the rate \n",
        "  # limitations of the pytumblr API, as discussed above.\n",
        "  while j < 25:\n",
        "\n",
        "    # Retrieves search results with the given tag.\n",
        "    search_results = client.tagged(tag, filter=filter, before=before)\n",
        "\n",
        "    # Iterates through the search results, appending them to the given file.\n",
        "    for i in search_results:\n",
        "      tags = (i)['tags']\n",
        "      f.write(' '.join(tags))\n",
        "      f.write('\\n')\n",
        "\n",
        "      # Records time of the last post in this iteration, so a unique post can\n",
        "      # be considered in the following one.\n",
        "      time_note = (i)['timestamp']\n",
        "\n",
        "    # Updates variables accordingly.\n",
        "    before = time_note\n",
        "    j = j+1"
      ],
      "metadata": {
        "id": "ONFyGLLpzFsY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrapes Tumblr for each of the 100 words on our list.\n",
        "for word in word_list:\n",
        "  scrape(word)"
      ],
      "metadata": {
        "id": "NhyK_mhcw4jx"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}